{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-Neural-Project.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1iKtdG1bjmPYlTsjo0xrRsh193titj7Cb",
          "timestamp": 1524713972512
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Oc44HjsnS5SG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ee7bcb7-e60a-4465-cec8-5e7a723fe24c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525058666601,
          "user_tz": 240,
          "elapsed": 6055,
          "user": {
            "displayName": "Abhishek Kumar",
            "photoUrl": "//lh6.googleusercontent.com/-1IIri8PuptI/AAAAAAAAAAI/AAAAAAAAElE/RMgSD-7u2Z4/s50-c-k-no/photo.jpg",
            "userId": "108365469511301265401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "urllib.urlretrieve (\"https://storage.googleapis.com/colab-sample-bucket-2f34f482-4b4a-11e8-8b65-0242ac110002/Thirdmodel.h5\", \"Thirdmodel.h5\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Thirdmodel.h5', <httplib.HTTPMessage instance at 0x7fa24bd25368>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "DXK614CHzOEz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5ca8c959-e96a-4338-f5b4-facb2aa5dc2c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525055940427,
          "user_tz": 240,
          "elapsed": 8384,
          "user": {
            "displayName": "Abhishek Kumar",
            "photoUrl": "//lh6.googleusercontent.com/-1IIri8PuptI/AAAAAAAAAAI/AAAAAAAAElE/RMgSD-7u2Z4/s50-c-k-no/photo.jpg",
            "userId": "108365469511301265401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import uuid\n",
        "from google.colab import auth\n",
        "\n",
        "\n",
        "def fileUploadToGCS(bucket_name='colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002'):\n",
        "  auth.authenticate_user()\n",
        "  project_id = 'api-project-834845844624'\n",
        "  !gcloud config set project {project_id}\n",
        "\n",
        "  # Make a unique bucket to which we'll upload the file.\n",
        "  # (GCS buckets are part of a single global namespace.)\n",
        "  #bucket_name = 'colab-sample-bucket-' + str(uuid.uuid1())\n",
        "  #print bucket_name\n",
        "\n",
        "  !gsutil cp CustomVGG_custom_loss.h5 gs://{bucket_name}/CustomVGG_custom_loss_trained1.h5\n",
        "    \n",
        "def fileDownloadFromGCS_1():\n",
        "  auth.authenticate_user()\n",
        "  project_id = 'api-project-834845844624'\n",
        "  !gcloud config set project {project_id}\n",
        "  !gsutil cp gs://\"colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\"/CustomVGGOrig_fullyConnected_retrained.h5 ./CustomVGGOrig_fullyConnected_retrained.h5\n",
        "\n",
        "def fileDownloadFromGCS():\n",
        "  auth.authenticate_user()\n",
        "  project_id = 'api-project-834845844624'\n",
        "  !gcloud config set project {project_id}\n",
        "  !gsutil cp gs://\"colab-sample-bucket-2a93eea2-49be-11e8-b0d5-0242ac110002\"/Pandora_18k.zip ./Pandora_18k.zip\n",
        "  !gsutil cp gs://\"colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\"/try1-dataset.sav ./try1-dataset.sav\n",
        "  #!gsutil cp gs://\"colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\"/CustomVGG.h5 ./CustomVGG.h5\n",
        "  !gsutil cp gs://\"colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\"/meanfeats_train_perclass.sav ./meanfeats_train_perclass.sav\n",
        "  #!gsutil cp gs://\"colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\"/VGG16_original.h5 ./VGG16_original.h5\n",
        "  !gsutil cp gs://\"colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\"/CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5 ./CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\n",
        "#fileUploadToGCS()\n",
        "#fileDownloadFromGCS()\n",
        "fileDownloadFromGCS_1()\n",
        "\"\"\"\n",
        "filenameTobucket_nameMap\n",
        "'Pandora_18k.zip':colab-sample-bucket-2a93eea2-49be-11e8-b0d5-0242ac110002\n",
        "'try1-dataset.sav': colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\n",
        "Now will use colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002  as the bucket id for all files\n",
        "CustomVGG.h5\n",
        "VGG16_original.h5\n",
        "CustomVGG_meanActivations_skipconn.h5\n",
        "CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\n",
        "\"\"\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002/CustomVGGOrig_fullyConnected_retrained.h5...\n",
            "\\ [1 files][260.4 MiB/260.4 MiB]                                                \n",
            "Operation completed over 1 objects/260.4 MiB.                                    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfilenameTobucket_nameMap\\n'Pandora_18k.zip':colab-sample-bucket-2a93eea2-49be-11e8-b0d5-0242ac110002\\n'try1-dataset.sav': colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002\\nNow will use colab-sample-bucket-7491b53e-49be-11e8-b0d5-0242ac110002  as the bucket id for all files\\nCustomVGG.h5\\nVGG16_original.h5\\nCustomVGG_meanActivations_skipconn.h5\\nCustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "xOF1gmybXgzs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "44585b91-e8d8-40e4-d689-ba590cbe6284",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525061875684,
          "user_tz": 240,
          "elapsed": 2843,
          "user": {
            "displayName": "Abhishek Kumar",
            "photoUrl": "//lh6.googleusercontent.com/-1IIri8PuptI/AAAAAAAAAAI/AAAAAAAAElE/RMgSD-7u2Z4/s50-c-k-no/photo.jpg",
            "userId": "108365469511301265401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip install pydot\n",
        "#!pip install graphviz\n",
        "import pydot\n",
        "import graphviz\n",
        "from keras.utils import plot_model\n",
        "def plotModel(model,filename):\n",
        "  plot_model(model, to_file=filename)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import pickle\n",
        "!ls -alh\n",
        "\n",
        "#!zip try1_weights.h5.zip try1_weights.h5\n",
        "#files.upload()\n",
        "#trainx,testx, trainy ,testy = pickle.load(open(\"try1-dataset.sav\", 'rb'))\n",
        "#print trainx.shape, testx.shape, trainy.shape,testy.shape\n",
        "#files.download('CustomVGG_custom_loss.png')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total 1.8G\r\n",
            "drwxr-xr-x  1 root root 4.0K Apr 30 03:24 .\r\n",
            "drwxr-xr-x  1 root root 4.0K Apr 30 01:07 ..\r\n",
            "drwx------  4 root root 4.0K Apr 30 01:10 .cache\r\n",
            "drwxr-xr-x  3 root root 4.0K Apr 30 01:10 .config\r\n",
            "-rw-r--r--  1 root root  91M Apr 30 02:37 CustomModelResNet_trained.h5\r\n",
            "-rw-r--r--  1 root root 426M Apr 30 01:16 CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\r\n",
            "-rw-r--r--  1 root root 261M Apr 30 02:38 CustomVGGOrig_fullyConnected_retrained.h5\r\n",
            "drwxr-xr-x  1 root root 4.0K Apr 30 01:12 datalab\r\n",
            "drwxr-xr-x  4 root root 4.0K Apr 30 01:08 .forever\r\n",
            "drwxr-xr-x  3 root root 4.0K Apr 30 01:13 .gsutil\r\n",
            "drwxr-xr-x  5 root root 4.0K Apr 30 01:10 .ipython\r\n",
            "drwxr-xr-x  2 root root 4.0K Apr 30 01:11 .keras\r\n",
            "drwx------  3 root root 4.0K Apr 30 01:08 .local\r\n",
            "-rw-r--r--  1 root root 730K Apr 30 02:37 meanfeats_train_perclass.sav\r\n",
            "drwx------  3 root root 4.0K Apr 30 01:17 .nv\r\n",
            "drwxr-xr-x 20 root root 4.0K Apr 30 01:16 Pandora_18k_merged\r\n",
            "-rw-r--r--  1 root root 1.4G Apr 30 02:37 Pandora_18k.zip_.gstmp\r\n",
            "-rw-------  1 root root 1.0K Apr 30 01:08 .rnd\r\n",
            "-rw-r--r--  1 root root 840M Apr 30 03:24 Thirdmodel.h5\r\n",
            "-rw-r--r--  1 root root 7.6M Apr 30 02:37 try1-dataset.sav\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vl8AT4v2ZK3v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#fileDownloadFromGCS()\n",
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('Pandora_18k.zip', 'r')\n",
        "zip_ref.extractall(\"./Pandora_18k\")\n",
        "zip_ref.close()\n",
        "import os\n",
        "datapath = './Pandora_18k'\n",
        "mergedDatapath = './Pandora_18k_merged'\n",
        "os.system(\"mkdir \"+mergedDatapath)\n",
        "def preprocess1():\n",
        "    d = datapath\n",
        "    dirsNames = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))]\n",
        "    dirs = [os.path.join(d,o) for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))]\n",
        "    #print dirs\n",
        "\n",
        "    # move all images from subfolders to a single folder. Ignore some naming conflicts\n",
        "    for i in xrange(len(dirs)):\n",
        "        mergedPath = mergedDatapath + '/' + dirsNames[i] + \"_merged\"\n",
        "        os.system(\"mkdir \"+mergedPath)\n",
        "        os.system(\"find \"+dirs[i]+\"  -iname '*.jpg' -exec cp -f -t \"+mergedPath+\"  '{}' +\")\n",
        "        #if i > 1: break\n",
        "preprocess1()\n",
        "!rm -rf Pandora_18k\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XBiJrHWDba4H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a22c145d-39e4-49ee-8f03-c5ee8151517e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525072679577,
          "user_tz": 240,
          "elapsed": 15429,
          "user": {
            "displayName": "Abhishek Kumar",
            "photoUrl": "//lh6.googleusercontent.com/-1IIri8PuptI/AAAAAAAAAAI/AAAAAAAAElE/RMgSD-7u2Z4/s50-c-k-no/photo.jpg",
            "userId": "108365469511301265401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "models = [load_model(\"CustomVGGOrig_fullyConnected_retrained.h5\"),load_model(\"CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\"),load_model(\"CustomModelResNet_trained.h5\")]\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kpraXWdGQUi4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "#from baseline import *\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "mergedDatapath = './Pandora_18k_merged'\n",
        "img_width,img_height = 224,224\n",
        "\n",
        "def createCustomModelResNet(num_classes=18):\n",
        "    #model = load_model(\"./CustomResNet.h5\")\n",
        "    model = ResNet50()\n",
        "    x = model.layers[-2].output\n",
        "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model_final = Model(input = model.input, output = predictions)\n",
        "    #print model.summary()\n",
        "    for layer in model_final.layers[:-1]:\n",
        "        layer.trainable = False\n",
        "    # compile the model \n",
        "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "    #print model_final.summary()\n",
        "    #model_final.save(\"./CustomResNet.h5\")\n",
        "    return model_final\n",
        "\n",
        "def createCustomModel(num_classes=18):\n",
        "    VGG_model_path = \"VGG16_original.h5\"\n",
        "    #model = VGG16(input_shape = (img_width, img_height, 3))\n",
        "    #model = load_model(VGG_model_path)\n",
        "    model = VGG16()\n",
        "    \"\"\"\n",
        "    image = load_img('239.jpg', target_size=(224, 224))\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # prepare the image for the VGG model\n",
        "    image = preprocess_input(image)\n",
        "    # predict the probability across all output classes\n",
        "    yhat = model.predict(image)\n",
        "    # convert the probabilities to class labels\n",
        "    label = decode_predictions(yhat)\n",
        "    # retrieve the most likely result, e.g. highest probability\n",
        "    label = label[0][0]\n",
        "    # print the classification\n",
        "    print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
        "    \"\"\"\n",
        "\n",
        "    #model.layers.pop()\n",
        "    x = model.layers[-5].output # dis regard the fc layers after this\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # creating the final model \n",
        "    model_final = Model(input = model.input, output = predictions)\n",
        "        #Total 24 layers,16 with weights\n",
        "    \"\"\"\n",
        "    [<keras.engine.topology.InputLayer at 0x7f0eb73f48d0>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb73f4a90>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb73f4ad0>,\n",
        "     <keras.layers.pooling.MaxPooling2D at 0x7f0eb73f4b50>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb73f4e90>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb73f4ed0>,\n",
        "     <keras.layers.pooling.MaxPooling2D at 0x7f0eb73f4fd0>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e150>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e190>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e290>,\n",
        "     <keras.layers.pooling.MaxPooling2D at 0x7f0eb639e390>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e4d0>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e510>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e610>,\n",
        "     <keras.layers.pooling.MaxPooling2D at 0x7f0eb639e710>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e850>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e890>,\n",
        "     <keras.layers.convolutional.Conv2D at 0x7f0eb639e990>,\n",
        "     <keras.layers.pooling.MaxPooling2D at 0x7f0eb639ea90>,  #19 till here freezing\n",
        "     <keras.layers.core.Flatten at 0x7f0eb4e3c3d0>,\n",
        "     <keras.layers.core.Dense at 0x7f0eb4e3c2d0>,\n",
        "     <keras.layers.core.Dropout at 0x7f0eb4e3c050>,\n",
        "     <keras.layers.core.Dense at 0x7f0eb4e64410>,\n",
        "     <keras.layers.core.Dense at 0x7f0eb4e64390>]\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    for layer in model_final.layers[:19]:\n",
        "      if i not in [5,8,11,13,16] and True:\n",
        "        layer.trainable = False\n",
        "      i += 1\n",
        "    # compile the model \n",
        "    #model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.adam(lr=0.0000005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "    print model_final.summary()\n",
        "    #model_final.save(\"./CustomVGG.h5\")\n",
        "    return model_final\n",
        "  \n",
        "\n",
        "\n",
        "def createCustomModel_meanfeats(inputdim=10,num_classes=18):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(1000, activation='relu', input_dim=inputdim))\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  model.add(Dense(num_classes,activation=\"softmax\"))\n",
        "  model.compile(loss = \"categorical_crossentropy\", optimizer = 'rmsprop', metrics=[\"accuracy\"])\n",
        "  return model\n",
        "from scipy.stats import mode\n",
        "def DataGenerator_test(X,size=(224,224),batch_size=20):\n",
        "    j  = 0\n",
        "    while 1:\n",
        "      idxs = np.arange(j,j+batch_size)\n",
        "      j += batch_size\n",
        "      images = []\n",
        "      for i in idxs:\n",
        "        if i >= X.shape[0]: break\n",
        "        images.append(img_to_array(load_img(X[i], target_size=size)))\n",
        "      images=np.array(images) \n",
        "      images=preprocess_input(images)\n",
        "      yield images # yield image and label pair\n",
        "      \n",
        "def runEnsembleModels(models, testx,testy,steps):\n",
        "    \n",
        "    pred=[model.predict_generator(DataGenerator_test(testx),steps=steps) for model in models] \n",
        "    #print pred[0].shape, pred[0],pred[1]\n",
        "    pred = [np.argmax(p,axis=1) for p in pred]\n",
        "    #print \"---\",pred[0],pred[0].shape\n",
        "    res = np.zeros((len(models),pred[0].shape[0]))\n",
        "    \n",
        "    for i in xrange(len(pred)):\n",
        "      for j in xrange(pred[i].shape[0]):\n",
        "        res[i][j]=pred[i][j]\n",
        "    #print res\n",
        "    maj=mode(res,axis=0)[0][0]\n",
        "    l = maj.shape[0]\n",
        "    print \"l=\",l\n",
        "    #print testy[:l]\n",
        "    print \"Accuracy of Ensemble:\",np.mean(maj==testy[:l])\n",
        "    return maj\n",
        "\n",
        "\n",
        "\n",
        "def getDataset2():\n",
        "    return pickle.load(open(\"try1-dataset.sav\", 'rb')) \n",
        "     \n",
        "  \n",
        "    '''returns only the files name and not the actual images'''\n",
        "    d = mergedDatapath\n",
        "    dirsNames = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))]\n",
        "    dirs = [os.path.join(d,o) for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))]\n",
        "\n",
        "    X,Y= [],[]\n",
        "    for i in range(len(dirs)):\n",
        "        d = dirs[i]\n",
        "        filenames = glob.glob(d+'/*.jpg')\n",
        "        l = len(filenames)\n",
        "\n",
        "        X += filenames\n",
        "        Y += [i for j in xrange(len(filenames))]\n",
        "    \n",
        "    l = len(X)\n",
        "    X= np.array(X)\n",
        "    Y=np.array(Y)\n",
        "    return train_test_split(X,Y,test_size=0.2)\n",
        "'''\n",
        "class DataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return np.array([\n",
        "            resize(imread(file_name), (200, 200))\n",
        "               for file_name in batch_x]), np.array(batch_y)\n",
        "'''\n",
        "\n",
        "def DataGenerator(X,Y,size=(224,224),batch_size=20):\n",
        "  while 1:\n",
        "\n",
        "    idxs = np.random.choice(X.shape[0],replace=False,size=batch_size)\n",
        "    images = []\n",
        "    for i in idxs:\n",
        "      images.append(img_to_array(load_img(X[i], target_size=size)))\n",
        "    images=np.array(images) \n",
        "    images=preprocess_input(images)\n",
        "    yield images,keras.utils.to_categorical(np.array(Y[idxs]),18) # yield image and label pair\n",
        "\n",
        "      \n",
        "\n",
        "      \n",
        "def DataGenerator_customlosswla(X,Y,size=(224,224),batch_size=20):\n",
        "  while 1:\n",
        "    idxs = np.random.choice(X.shape[0],replace=False,size=batch_size)\n",
        "    images = []\n",
        "    for i in idxs:\n",
        "      images.append(img_to_array(load_img(X[i], target_size=size)))\n",
        "    images=np.array(images) \n",
        "    images=preprocess_input(images)\n",
        "    yield [images,meanfeats_train_perclass[Y[idxs]]],[keras.utils.to_categorical(np.array(Y[idxs]),18),np.zeros(batch_size)] # yield image and label pair\n",
        "\n",
        "from keras import backend as KK\n",
        "def getIntermediateoutputs(functor,data=None):\n",
        "  \n",
        " # evaluation function\n",
        "  #print inp.shape, \"inp.shape\"\n",
        "  # Testing\n",
        "  test = data#np.random.randn(20,224,224,3)\n",
        "  layer_outs = functor([test, 1.])\n",
        "  #print type(layer_outs[0]),\"----\",layer_outs[0].shape\n",
        "  l = len(layer_outs)#.shape[0]\n",
        "  tempfeat = []\n",
        "  print \"len=\",l\n",
        "  for i in xrange(l):\n",
        "    #print \"Layer \", layeridxs[i],\" output shape=\",layer_outs[i].shape, np.sum(np.sum(layer_outs[i],axis=1),axis=1).shape\n",
        "    tempfeat.append(np.sum(np.sum(layer_outs[i],axis=1),axis=1))\n",
        "  meanfeat =  np.hstack(tempfeat)\n",
        "  #print meanfeat.shape, \" meanfeat.shape\"\n",
        "  return meanfeat\n",
        "  \n",
        "def saveMeanfeatDataset(X,Y,model,filename,size=(224,224),batch_size=20):\n",
        "    #get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],[model.layers[3].output])\n",
        "    #[1,2,4,5,7,8,9,11,12,13,14,15,16]\n",
        "    layeridxs = [5,8,11,13,16]\n",
        "    #layerlimit = [30,40,50,50,50]\n",
        "    inp = model.input  # input placeholder\n",
        "    outputs = []          # all layer outputs\n",
        "    sm = 0\n",
        "    for i in layeridxs:\n",
        "      sm += model.layers[i].output.shape.as_list()[-1]\n",
        "      outputs.append(model.layers[i].output)\n",
        "    functor = KK.function([inp]+ [KK.learning_phase()], outputs )\n",
        "    \n",
        "    batch = batch_size\n",
        "    i = 0\n",
        "    N = X.shape[0]\n",
        "    meanfeats = np.zeros((18,sm))\n",
        "    #print \"meanfeats.shape=\",meanfeats.shape\n",
        "    classcounts = np.zeros((18,1)) +0.01\n",
        "    np.add.at(classcounts[:,0],Y,1)\n",
        "    print classcounts, \"=classcounts\"\n",
        "    \n",
        "    while i < N:\n",
        "      images = []\n",
        "      for j in xrange(i,i+batch):\n",
        "        if j >= N :break\n",
        "        images.append(img_to_array(load_img(X[j], target_size=size)))\n",
        "       \n",
        "      images=np.array(images) \n",
        "      images=preprocess_input(images)\n",
        "      #print \"Images.shape:,\",images.shape\n",
        "      z=getIntermediateoutputs(functor,images)\n",
        "      print z.shape,\" zshape\"\n",
        "      np.add.at(meanfeats,Y[i:i+batch],z)\n",
        "      del images\n",
        "      i += batch\n",
        "      print \"Done with,\",i ,\" images.\"\n",
        "    meanf = meanfeats/classcounts\n",
        "    print \"finally:\",meanf.shape\n",
        "    pickle.dump(meanf, open(filename, 'wb'))\n",
        "    \n",
        "\n",
        "def trainVGGTypeModel(model,savename=None):\n",
        "  trainx,testx, trainy ,testy = getDataset2()\n",
        "  #print trainx.shape, testx.shape\n",
        "  #model = createCustomModel()\n",
        "  \n",
        "  #print model.summary()\n",
        "  print \"Traing the custom model\"\n",
        "  batch = 20\n",
        "  #model.fit(trainx, trainy,batch_size=batch,nb_epoch=20,shuffle=True,verbose=1,validation_data=(testx, testy))\n",
        "  #model.fit_generator(DataGenerator_customlosswla(trainx,trainy,batch_size=batch),epochs=20,steps_per_epoch=650,verbose=1,validation_data=DataGenerator_customlosswla(testx,testy,batch_size=batch),validation_steps=176)\n",
        "  model.fit_generator(DataGenerator(trainx,trainy,batch_size=batch),epochs=40,steps_per_epoch=650,verbose=1,validation_data=DataGenerator(testx,testy,batch_size=batch),validation_steps=176)\n",
        "  if savename != None:\n",
        "    model.save(savename)\n",
        "      \n",
        "def trainMeanfeatModel():\n",
        "  trainx,trainy = pickle.load(open(\"meanfeats_trainx_y.sav\", 'rb'))\n",
        "  testx,testy = pickle.load(open(\"meanfeats_testx_y.sav\", 'rb'))\n",
        "  trainy = keras.utils.to_categorical(trainy,18)\n",
        "  testy = keras.utils.to_categorical(testy,18)\n",
        "  print trainx.shape\n",
        "  model = createCustomModel_meanfeats(inputdim=250,num_classes=18)\n",
        "  print model.summary()\n",
        "  model.fit(trainx, trainy,batch_size=20,nb_epoch=20,shuffle=True,verbose=1,validation_data=(testx, testy))\n",
        "  model.save(\"./abhi_mean_feat_model.h5\")\n",
        "\n",
        "from keras import backend as KK\n",
        "def createCustomModelVGG3(num_classes=18):\n",
        "    VGG_model_path = \"VGG16_original.h5\"\n",
        "    model = load_model(VGG_model_path)\n",
        "    #model = VGG16()\n",
        "    #model.save(VGG_model_path)\n",
        "    #sys.exit(0)\n",
        "\n",
        "    #[1,2,4,5,7,8,9,11,12,13,14,15,16]\n",
        "    layeridxs = [5,7,8,9,11,12,13,14,15,16,18]\n",
        "    for i in layeridxs:\n",
        "      print \"i=\",i,model.layers[i].output.shape\n",
        "    #print keras.layers.add([model.layers[4].output,model.layers[7].output]).output.shape\n",
        "    #print KK.sum()[KK.sum(model.layers[4].output,axis=1),axis=1).shape\n",
        "    #sys.exit(0)\n",
        "    def f(x):\n",
        "      from keras import backend as KK\n",
        "      return  KK.mean(KK.mean(x,axis=1),axis=1)\n",
        "    #x = KK.concatenate([KK.sum(KK.sum(model.layers[i].output,axis=1),axis=1) for i in layeridxs])\n",
        "    #print \"x.shape=\",x.shape #x.shape= (?, 2240)\n",
        "    #flatmeanx = [Flatten()(model.layers[i].output) for i in layeridxs]\n",
        "    #x = keras.layers.concatenate(flatx,axis=-1)\n",
        "    \n",
        "    \n",
        "    flatx = [keras.layers.Lambda(f)(model.layers[i].output) for i in layeridxs]\n",
        "    #flatx.append(keras.layers.Lambda(f)(model.layers[18].output))\n",
        "    for i in flatx:\n",
        "      print i.shape\n",
        "    #x = keras.layers.concatenate(flatx,axis=-1)\n",
        "    #print x.shape,\" x.shape\"\n",
        "    #sys.exit(0)\n",
        "\n",
        "    flx = keras.layers.concatenate(flatx,axis=-1)\n",
        "    x = Dense(256, activation=\"relu\")(flx)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # creating the final model \n",
        "    model_final = Model(input = model.input, output = predictions)\n",
        "    print \"total layers=\", len(model_final.layers)\n",
        "    for i,layer in enumerate(model_final.layers[:16]):\n",
        "       layer.trainable = False\n",
        "    # compile the model \n",
        "    #model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "    print model_final.summary()\n",
        "    #sys.exit(0)\n",
        "    model_final.save(\"./CustomVGG_meanActivations_skipconn.h5\")\n",
        "    return model_final\n",
        "  \n",
        "meanfeats_train_perclass = pickle.load(open(\"meanfeats_train_perclass.sav\", 'rb'))\n",
        "from keras import losses\n",
        "def mycustomloss(y_true, y_pred):\n",
        "  print y_pred.shape\n",
        "  return KK.sqrt(KK.sum(y_pred)) # ytrue will be 0 and ypred will be squared diff\n",
        "  \n",
        "def createCustomModelVGG2(num_classes=18):\n",
        "    \n",
        "    VGG_model_path = \"VGG16_original.h5\"\n",
        "    model = load_model(VGG_model_path)\n",
        "    #print model.summary()\n",
        "    #model = VGG16()\n",
        "    #model.save(VGG_model_path)\n",
        "    #sys.exit(0)\n",
        "\n",
        "    #[1,2,4,5,7,8,9,11,12,13,14,15,16]\n",
        "    \n",
        "    layeridxs = [5,8,11,13,16]\n",
        "    for i in layeridxs:\n",
        "      print \"i=\",i,model.layers[i].output.shape\n",
        "    #print keras.layers.add([model.layers[4].output,model.layers[7].output]).output.shape\n",
        "    #print KK.sum()[KK.sum(model.layers[4].output,axis=1),axis=1).shape\n",
        "    #sys.exit(0)\n",
        "    #meanfeats_train_perclass_tf = KK.cast_to_floatx(meanfeats_train_perclass)\n",
        "    def f(x):\n",
        "      from keras import backend as KK\n",
        "      return  KK.mean(KK.mean(x,axis=1),axis=1)\n",
        "    \n",
        "    def f2(a):\n",
        "      return KK.abs(a)\n",
        "    #x = KK.concatenate([KK.sum(KK.sum(model.layers[i].output,axis=1),axis=1) for i in layeridxs])\n",
        "    #print \"x.shape=\",x.shape #x.shape= (?, 2240)\n",
        "    #flatmeanx = [Flatten()(model.layers[i].output) for i in layeridxs]\n",
        "    #x = keras.layers.concatenate(flatx,axis=-1)\n",
        "    \n",
        "    \n",
        "    flatx = [keras.layers.Lambda(f)(model.layers[i].output) for i in layeridxs]\n",
        "    #flatx.append(keras.layers.Lambda(f)(model.layers[18].output))\n",
        "    for i in flatx:\n",
        "      print i.shape\n",
        "    #x = keras.layers.concatenate(flatx,axis=-1)\n",
        "    #print x.shape,\" x.shape\"\n",
        "    #sys.exit(0)\n",
        "    flx = keras.layers.concatenate(flatx,axis=-1)\n",
        "    auxiliary_input = keras.layers.Input(shape=(meanfeats_train_perclass.shape[1],), name='aux_input')\n",
        "    flx = keras.layers.Subtract()([flx,auxiliary_input])\n",
        "    flx = keras.layers.Lambda(f2,name='activationDiff')(flx)\n",
        "    \n",
        "    \n",
        "    ix = model.layers[18].output\n",
        "    #print type(ix), ix.shape,\"ix shape\"\n",
        "    ix = Flatten()(ix)\n",
        "    x = Dense(1024, activation=\"relu\")(ix)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(num_classes, activation=\"softmax\",name='LabelDiff')(x)\n",
        "\n",
        "    # creating the final model \n",
        "    model_final = Model(input = [model.input,auxiliary_input], output = [predictions,flx])\n",
        "    print \"total layers=\", len(model_final.layers)\n",
        "    for i,layer in enumerate(model_final.layers[:18]):\n",
        "       if i not in layeridxs:\n",
        "        layer.trainable = False\n",
        "    # compile the model \n",
        "    #model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "    model_final.compile(loss = [\"categorical_crossentropy\",mycustomloss], optimizer = optimizers.adam(lr=0.0000005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0),metrics=[\"accuracy\"],loss_weights=[1,0])\n",
        "\n",
        "\n",
        "    print model_final.summary()\n",
        "    #sys.exit(0)\n",
        "    #model_final.save(\"./CustomVGG_meanActivations_hackk_customloss.h5\")\n",
        "    return model_final\n",
        "def dopredictions(model):\n",
        "    trainx,testx, trainy ,testy = getDataset2()\n",
        "    print model.metrics_names\n",
        "    print model.evaluate_generator(DataGenerator(testx,testy,batch_size=20),steps=176)\n",
        "\n",
        "def domeanfeats(model):\n",
        "  trainx,testx, trainy ,testy = getDataset2()\n",
        "  saveMeanfeatDataset(trainx,trainy,model,\"meanfeats_train_perclass.sav\",batch_size=100)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    #model = load_model(\"./CustomVGG.h5\")\n",
        "    #createCustomModelVGG3()\n",
        "    #sys.exit(0)\n",
        "    #model1 = load_model(\"./CustomVGG_meanActivations_skipconn_trained_9_14_16.h5\")\n",
        "    #model = createCustomModelVGG2()\n",
        "    #model.set_weights(model1.get_weights())\n",
        "    #trainMeanfeatModel()\n",
        "    #dopredictions(load_model(\"./CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\"))\n",
        "    #trainVGGTypeModel(createCustomModelResNet(),\"CustomModelResNet_trained.h5\")\n",
        "    \n",
        "    #customlossmodel = createCustomModelVGG2()\n",
        "    #customlossmodel.set_weights(load_model(\"CustomVGGOrig_fullyConnected_retrained.h5\").get_weights())\n",
        "    #trainVGGTypeModel(customlossmodel,\"CustomVGG_custom_loss_trained.h5\")\n",
        "    #trainVGGTypeModel(createCustomModel(),\"CustomVGG_orig_trained.h5\")\n",
        "    #saveMeanfeatDataset(trainx,trainy,model,\"meanfeats_trainx_y.sav\")\n",
        "    #saveMeanfeatDataset(testx,testy,model,\"meanfeats_testx_y.sav\")\n",
        "    #domeanfeats(load_model(\"./CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\"))\n",
        "\n",
        "    #plotModel(createCustomModelVGG2(),'CustomVGG_custom_loss.png')\n",
        "    \n",
        "    #dopredictions(models[0])\n",
        "    #dopredictions(models[0])\n",
        "    #trainx,testx, trainy ,testy = getDataset2()\n",
        "    #print testy.shape\n",
        "    #runEnsembleModels([models[0],models[1],models[2],load_model('Thirdmodel.h5')],testx,testy,steps=170)\n",
        "    #sys.exit(0)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqkE53ZOXWZN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "271eafe8-fc50-4683-f356-462699acdc44"
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "trainx,testx, trainy ,testy = getDataset2()\n",
        "print testy.shape\n",
        "y_pred = runEnsembleModels([models[0],models[1],models[2]],testx,testy,steps=170)\n",
        "#sys.exit(0)\n",
        "import os\n",
        "d= \"Pandora_18k_merged/\"\n",
        "dirsNames = [o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))]\n",
        "classes = [i[3:] for i in dirsNames]\n",
        "print classes, len(classes)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(testy[:y_pred.shape[0]], y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "#plt.figure()\n",
        "#plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plt.savefig(\"confusion_matrix_ensemble.jpeg\")\n",
        "plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
        "                      title='Normalized confusion matrix for final Ensemble.')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4kZPg-93GYPd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as KK\n",
        "!apt-get install graphviz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NBr_GYfxQa_q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "50096f46-3975-4a6a-e637-3673bb026233",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525072953120,
          "user_tz": 240,
          "elapsed": 1536,
          "user": {
            "displayName": "Abhishek Kumar",
            "photoUrl": "//lh6.googleusercontent.com/-1IIri8PuptI/AAAAAAAAAAI/AAAAAAAAElE/RMgSD-7u2Z4/s50-c-k-no/photo.jpg",
            "userId": "108365469511301265401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -alh\n",
        "\n",
        "'''\n",
        "def createCustomModelVGG2(num_classes=18):\n",
        "    VGG_model_path = \"VGG16_original.h5\"\n",
        "    model = load_model(VGG_model_path)\n",
        "    #model = VGG16()\n",
        "    #model.save(VGG_model_path)\n",
        "    #sys.exit(0)\n",
        "\n",
        "    #[1,2,4,5,7,8,9,11,12,13,14,15,16]\n",
        "    layeridxs = [9,14,16]\n",
        "    for i in layeridxs:\n",
        "      print \"i=\",i,model.layers[i].output.shape\n",
        "    #print keras.layers.add([model.layers[4].output,model.layers[7].output]).output.shape\n",
        "    #print KK.sum()[KK.sum(model.layers[4].output,axis=1),axis=1).shape\n",
        "    #sys.exit(0)\n",
        "    def f(x):\n",
        "      from keras import backend as KK\n",
        "      return  KK.mean(KK.mean(x,axis=1),axis=1)\n",
        "    #x = KK.concatenate([KK.sum(KK.sum(model.layers[i].output,axis=1),axis=1) for i in layeridxs])\n",
        "    #print \"x.shape=\",x.shape #x.shape= (?, 2240)\n",
        "    #flatmeanx = [Flatten()(model.layers[i].output) for i in layeridxs]\n",
        "    #x = keras.layers.concatenate(flatx,axis=-1)\n",
        "    \n",
        "    \n",
        "    flatx = [keras.layers.Lambda(f)(model.layers[i].output) for i in layeridxs]\n",
        "    #flatx.append(keras.layers.Lambda(f)(model.layers[18].output))\n",
        "    for i in flatx:\n",
        "      print i.shape\n",
        "    #x = keras.layers.concatenate(flatx,axis=-1)\n",
        "    #print x.shape,\" x.shape\"\n",
        "    #sys.exit(0)\n",
        "    ix = model.layers[18].output\n",
        "    ix = Flatten()(ix)\n",
        "    x = Dense(1024, activation=\"relu\")(ix)\n",
        "    x = Dropout(0.5)(x)\n",
        "    flatx.append(x)\n",
        "    flx = keras.layers.concatenate(flatx,axis=-1)\n",
        "    x = Dense(1024, activation=\"relu\")(flx)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    # creating the final model \n",
        "    model_final = Model(input = model.input, output = predictions)\n",
        "    print \"total layers=\", len(model_final.layers)\n",
        "    for i,layer in enumerate(model_final.layers[:16]):\n",
        "       layer.trainable = False\n",
        "    # compile the model \n",
        "    #model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.adam(lr=0.000005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "    print model_final.summary()\n",
        "    model_final.save(\"./CustomVGG_meanActivations_skipconn.h5\")\n",
        "    return model_final\n",
        "'''\n",
        "from google.colab import files\n",
        "#files.download('CustomVGG_meanActivations_skipconn_trained2_9_14_16.png')\n",
        "#files.download('https://storage.googleapis.com/colab-sample-bucket-2f34f482-4b4a-11e8-8b65-0242ac110002/Thirdmodel.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.8G\r\n",
            "drwxr-xr-x  1 root root 4.0K Apr 30 07:22 .\r\n",
            "drwxr-xr-x  1 root root 4.0K Apr 30 01:07 ..\r\n",
            "drwx------  4 root root 4.0K Apr 30 01:10 .cache\r\n",
            "drwxr-xr-x  3 root root 4.0K Apr 30 01:10 .config\r\n",
            "-rw-r--r--  1 root root 4.2K Apr 30 07:22 confusion_matrix_ensemble.jpeg\r\n",
            "-rw-r--r--  1 root root  91M Apr 30 02:37 CustomModelResNet_trained.h5\r\n",
            "-rw-r--r--  1 root root 426M Apr 30 01:16 CustomVGG_meanActivations_skipconn_trained2_9_14_16.h5\r\n",
            "-rw-r--r--  1 root root 261M Apr 30 02:38 CustomVGGOrig_fullyConnected_retrained.h5\r\n",
            "drwxr-xr-x  1 root root 4.0K Apr 30 01:12 datalab\r\n",
            "drwxr-xr-x  4 root root 4.0K Apr 30 01:08 .forever\r\n",
            "drwxr-xr-x  3 root root 4.0K Apr 30 01:13 .gsutil\r\n",
            "drwxr-xr-x  5 root root 4.0K Apr 30 01:10 .ipython\r\n",
            "drwxr-xr-x  3 root root 4.0K Apr 30 04:20 .keras\r\n",
            "drwx------  3 root root 4.0K Apr 30 01:08 .local\r\n",
            "-rw-r--r--  1 root root 730K Apr 30 02:37 meanfeats_train_perclass.sav\r\n",
            "drwx------  3 root root 4.0K Apr 30 01:17 .nv\r\n",
            "drwxr-xr-x 20 root root 4.0K Apr 30 01:16 Pandora_18k_merged\r\n",
            "-rw-r--r--  1 root root 1.4G Apr 30 02:37 Pandora_18k.zip_.gstmp\r\n",
            "-rw-------  1 root root 1.0K Apr 30 01:08 .rnd\r\n",
            "-rw-r--r--  1 root root 840M Apr 30 03:24 Thirdmodel.h5\r\n",
            "-rw-r--r--  1 root root 7.6M Apr 30 02:37 try1-dataset.sav\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AAtQ00SFVkVk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab842f77-a96d-42a7-e7c2-7cf39d21ae0c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524953943491,
          "user_tz": 240,
          "elapsed": 301,
          "user": {
            "displayName": "Abhishek Kumar",
            "photoUrl": "//lh6.googleusercontent.com/-1IIri8PuptI/AAAAAAAAAAI/AAAAAAAAElE/RMgSD-7u2Z4/s50-c-k-no/photo.jpg",
            "userId": "108365469511301265401"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#print type(pickle.load(open(\"meanfeats_train_perclass.sav\", 'rb')))\n",
        "\n",
        "'''\n",
        "models=[model1,model3]\n",
        "    model_input = Input(shape=models[0].input_shape[1:])\n",
        "    def ensembleModels(models, model_input):\n",
        "    yModels=[model(model_input) for model in models] \n",
        "    yAvg=keras.layers.average(yModels) \n",
        "    modelEns = Model(inputs=model_input, outputs=yAvg,    name='ensemble')\n",
        "    \n",
        "    return modelEns\n",
        "    model_ensemble = ensembleModels(models, model_input)\n",
        "    model_ensemble.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.00001, momentum=0.5), metrics=[\"accuracy\"])\n",
        "    score = model_ensemble.evaluate_generator(DataGenerator(testx,testy), steps=1)\n",
        "    print score\n",
        "    \n",
        "'''\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<type 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}